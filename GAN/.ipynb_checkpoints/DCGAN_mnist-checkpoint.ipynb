{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenno/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "from tensorflow.keras.layers import Input,ELU,Flatten,Dense,LeakyReLU, Reshape\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization,Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The results are a little better when the dimensionality of the random vector is only 10.\n",
    "# The dimensionality has been left at 100 for consistency with other GAN implementations.\n",
    "randomDim = 100\n",
    "\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data('/home/jenno/Desktop/data/mnist/mnist.npz')\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "\n",
    "#reshape X_train from (n, 28, 28) to (n, 28, 28, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "\n",
    "# Optimizer\n",
    "adam = Adam(lr = 0.0002, beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "generator = Sequential()\n",
    "\n",
    "generator.add(Conv2DTranspose(filters=256, kernel_size= (7,7), input_shape=(1,1, randomDim)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(ELU())\n",
    "\n",
    "generator.add(Conv2DTranspose(filters=128, kernel_size= (5,5), strides= (2,2), padding= 'same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(ELU())\n",
    "\n",
    "generator.add(Conv2DTranspose(filters=1,kernel_size= (5,5),strides= (2,2),activation='tanh',padding= 'same'))\n",
    "#generator.compile(loss='binary_crossentropy', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "#discriminator = Sequential()\n",
    "#discriminator.add(Conv2D(32, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)))\n",
    "#discriminator.add(BatchNormalization())\n",
    "#discriminator.add(ELU())\n",
    "\n",
    "#discriminator.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "#discriminator.add(BatchNormalization())\n",
    "#discriminator.add(ELU())\n",
    "\n",
    "#discriminator.add(Conv2D(1, kernel_size=(7, 7), activation='sigmoid'))\n",
    "#discriminator.add(Flatten())\n",
    "#discriminator.add(Dense(1028))\n",
    "#discriminator.add(ELU())\n",
    "\n",
    "#discriminator.add(Dense(1, activation='sigmoid'))\n",
    "#discriminator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Flatten(input_shape=(28, 28, 1)))\n",
    "discriminator.add(Dense(1024))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined network\n",
    "\n",
    "ganInput = Input(shape=(1,1, randomDim))\n",
    "x = generator(ganInput)\n",
    "ganOutput = discriminator(x)\n",
    "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "dLosses = []\n",
    "gLosses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss from each batch\n",
    "def plotLoss(epoch):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(dLosses, label='Discriminitive loss')\n",
    "    plt.plot(gLosses, label='Generative loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('reference/images/DCgan_loss_epoch_%d.png' % epoch)\n",
    "\n",
    "# Create a wall of generated MNIST images\n",
    "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, 1, 1, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generatedImages.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reference/images/DCgan_generated_image_epoch_%d.png' % epoch)\n",
    "\n",
    "# Save the generator and discriminator networks (and weights) for later use\n",
    "def saveModels(epoch):\n",
    "    generator.save('reference/models/DCgan_generator_epoch_%d.h5' % epoch)\n",
    "    discriminator.save('reference/models/DCgan_discriminator_epoch_%d.h5' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=1, batchSize=128):\n",
    "    batchCount = X_train.shape[0] // batchSize\n",
    "    for e in range(epochs):\n",
    "        epoch_dLoss = []\n",
    "        epoch_gLoss = []\n",
    "        for i in range(batchCount):\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, 1, 1, randomDim])\n",
    "            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n",
    "\n",
    "            # Generate fake MNIST images\n",
    "            generatedImages = generator.predict(noise)\n",
    "            # print np.shape(imageBatch), np.shape(generatedImages)\n",
    "            X = np.concatenate([imageBatch, generatedImages])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            yDis = np.zeros(2*batchSize)\n",
    "            # One-sided label smoothing\n",
    "            yDis[:batchSize] = 0.9\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            dloss = discriminator.train_on_batch(X, yDis)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, 1, 1, randomDim])\n",
    "            yGen = np.ones(batchSize)\n",
    "            discriminator.trainable = False\n",
    "            gloss = gan.train_on_batch(noise, yGen)\n",
    "\n",
    "            epoch_dLoss.append(dloss)\n",
    "            epoch_gLoss.append(gloss)\n",
    "        average_dloss = np.mean(epoch_dLoss)\n",
    "        average_gloss = np.mean(epoch_gLoss)\n",
    "        print('Epoch %d' % e, flush= True)\n",
    "        print('generator loss: ' + str(average_gloss),flush= True)\n",
    "        print('discriminator loss: ' + str(average_dloss),flush= True)\n",
    "        dLosses.append(average_dloss)\n",
    "        gLosses.append(average_gloss)\n",
    "        if e % 100 == 0 and e != 0:\n",
    "            plotGeneratedImages(e)\n",
    "            #saveModels(e)\n",
    "\n",
    "    # Plot losses from every epoch\n",
    "    plotLoss(e)\n",
    "    saveModels(e)\n",
    "    print('training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Total params: 2,920,450\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 1,460,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 256)         1254656   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "elu (ELU)                    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         3201      \n",
      "=================================================================\n",
      "Total params: 2,078,721\n",
      "Trainable params: 2,077,953\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "generator loss: 0.60120827\n",
      "discriminator loss: 0.4845387\n"
     ]
    }
   ],
   "source": [
    "train(500, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
